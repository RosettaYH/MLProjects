{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alternative-latvia",
   "metadata": {},
   "source": [
    "# Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-teacher",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv('True.csv')\n",
    "fake_df = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df['category'] = 1\n",
    "fake_df['category'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([true_df, fake_df])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-raleigh",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['category'], label='Count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='subject', hue='category', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'] + \" \" + df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop.update(punctuation)\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wc(text):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=STOPWORDS).generate(text)\n",
    "    plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True\n",
    "show_wc(\" \".join(df[df.category == 1].text))\n",
    "# Fake\n",
    "show_wc(\" \".join(df[df.category == 0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_text_ngrams(corpus, n, g):\n",
    "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def show_ngrams(n):\n",
    "    plt.figure(figsize=(16,9))\n",
    "    most_common = get_top_text_ngrams(df.text, 10, n)\n",
    "    most_common = dict(most_common)\n",
    "    sns.barplot(x=list(most_common.values()), y=list(most_common.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "show_ngrams(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram\n",
    "show_ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "show_ngrams(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-oxygen",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "max_length = 300\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 40000 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n",
    "X_train = training_padded\n",
    "X_test = testing_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.twitter.27B.100d.txt'\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "nb_words = min(vocab_size, len(word_index))\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocab_size: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-moses",
   "metadata": {},
   "source": [
    "### Models Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(128, return_sequences=True, recurrent_dropout=0.25, dropout=0.25))\n",
    "model.add(LSTM(64, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train,  y_train, validation_data=(X_test, y_test), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc)) \n",
    "\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(cm, cmap='Blues', linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=['Fake', 'Original'], yticklabels=['Fake', 'Original'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inspiration\n",
    "1. https://www.kaggle.com/madz2000/nlp-using-glove-embeddings-99-87-accuracy\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
