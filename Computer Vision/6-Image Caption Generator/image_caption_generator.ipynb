{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convertible-curve",
   "metadata": {},
   "source": [
    "# Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-bacon",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img_name):\n",
    "    image = load_img(img_name, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(img_dir):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    print(model.summary())\n",
    "\n",
    "    features = dict()\n",
    "    for name in tqdm(os.listdir(img_dir)):\n",
    "        img_name = os.path.join(img_dir, name)\n",
    "        image = preprocess_img(img_name)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "    \n",
    "    print('Extracted Features: %d' % len(features))\n",
    "    dump(features, open('features.pkl', 'wb'))\n",
    "\n",
    "prepare_image('Flickr8k_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(filename):\n",
    "    text = load_doc(filename)\n",
    "\n",
    "    # Load\n",
    "    desc = dict()\n",
    "    for line in text.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in desc:\n",
    "            desc[image_id] = list()\n",
    "        desc[image_id].append(image_desc)\n",
    "    \n",
    "    # Clean\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in desc.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            d = desc_list[i]\n",
    "            d = d.split()\n",
    "            d = [word.lower() for word in d]\n",
    "            d = [w.translate(table) for w in d]\n",
    "            d = [word for word in d if len(word)>1]\n",
    "            d = [word for word in d if word.isalpha()]\n",
    "            desc_list[i] =  ' '.join(d)\n",
    "    \n",
    "    # Save\n",
    "    lines = list()\n",
    "    for key, desc_list in desc.items():\n",
    "        for d in desc_list:\n",
    "            lines.append(key + ' ' + d)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open('descriptions.txt', 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "prepare_text('Flickr8k_text/Flickr8k.token.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-specialist",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatset(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_feat(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def get_max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_datatset('Flickr8k_text/Flickr_8k.trainImages.txt')\n",
    "print('Train Dataset: %d' % len(train))\n",
    "train_desc = load_text('descriptions.txt', train)\n",
    "print('Train Descriptions: %d' % len(train_desc))\n",
    "train_feat = load_img_feat('features.pkl', train)\n",
    "print('Train Images: %d' % len(train_feat))\n",
    "\n",
    "test = load_datatset('Flickr8k_text/Flickr_8k.devImages.txt')\n",
    "print('Test Dataset: %d' % len(test))\n",
    "test_desc = load_text('descriptions.txt', test)\n",
    "print('Test Descriptions: %d' % len(test_desc))\n",
    "test_feat = load_img_feat('features.pkl', test)\n",
    "print('Test Images: %d' % len(test_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_desc)\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "max_length = get_max_length(train_desc)\n",
    "print('Max description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train, y_train = create_sequences(tokenizer, max_length, train_desc, train_feat, vocab_size)\n",
    "X1_test, X2_test, y_test = create_sequences(tokenizer, max_length, test_desc, test_feat, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-discovery",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    # Feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # Sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # Decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps = len(train_desc)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(tokenizer, max_length, train_desc, train_feat, vocab_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps)\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-nutrition",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, max_length, photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        seq = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=max_length)\n",
    "\n",
    "        y_pred = model.predict([photo, padded_seq], verbose=0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        word = word_for_id(y_pred, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, max_length, descriptions, photos):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        y_pred = generate_desc(model, tokenizer, max_length, photos[key])\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(y_pred.split())\n",
    "        \n",
    "    # BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_{}.h5'.format(epochs-1)\n",
    "model = load_model(model_name)\n",
    "evaluate_model(model, tokenizer, max_length, test_desc, test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-button",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_name):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    image = preprocess_img(img_name)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "model = load_model(model_name)\n",
    "img_name = 'a.jpg'\n",
    "image_feature = extract_features(img_name)\n",
    "predict_desc = generate_desc(model, tokenizer, max_length, image_feature)\n",
    "print(\"\\n\", predict_desc, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread(img_name)\n",
    "plt.imshow(img)\n",
    "predict_desc_clean = predict_desc.replace('startseq', '')\n",
    "predict_desc_clean = predict_desc_clean.replace('endseq', '')\n",
    "plt.title(predict_desc_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inspiration\n",
    "1. https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
